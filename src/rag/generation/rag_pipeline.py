from typing import List, Dict, Optional
from src.rag.generation.llm_handler import OllamaLLM
from src.rag.generation.retriever import Retriever

class RAGPipeline:
    """
    Pipeline RAG complet: Retrieval + Generation
    """
    
    def __init__(
        self,
        retriever: Retriever,
        llm: OllamaLLM,
        system_prompt: Optional[str] = None
    ):
        """
        Args:
            retriever: Syst√®me de r√©cup√©ration
            llm: Mod√®le de langage Ollama
            system_prompt: Instructions syst√®me personnalis√©es
        """
        self.retriever = retriever
        self.llm = llm
        
        self.system_prompt = system_prompt or self._default_system_prompt()
    
    def _default_system_prompt(self) -> str:
        """Prompt syst√®me par d√©faut pour ESILV"""
        return """Tu es un assistant intelligent pour l'√©cole d'ing√©nieurs ESILV.

INSTRUCTIONS:
- R√©ponds UNIQUEMENT en te basant sur le CONTEXTE fourni
- Sois pr√©cis, factuel et professionnel
- Si l'information n'est PAS dans le contexte, dis "Je n'ai pas cette information dans ma base de donn√©es"
- Ne JAMAIS inventer d'informations
- Utilise un ton amical mais professionnel
- R√©ponds en fran√ßais sauf si la question est en anglais

CONTEXTE:
{context}

QUESTION: {query}

R√âPONSE:"""
    
    def _format_context(self, chunks: List[Dict]) -> str:
        """
        Formate les chunks r√©cup√©r√©s en contexte
        
        Args:
            chunks: Chunks r√©cup√©r√©s
            
        Returns:
            Contexte format√©
        """
        if not chunks:
            return "Aucun contexte disponible."
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            source = chunk['metadata'].get('source', 'Inconnu')
            score = chunk.get('similarity_score', 0)
            content = chunk['content'].strip()
            
            context_parts.append(
                f"[Source {i}: {source} (Score: {score:.2f})]\n{content}\n"
            )
        
        return "\n---\n".join(context_parts)
    
    def query(
        self,
        user_query: str,
        return_sources: bool = True,
        stream: bool = False
    ) -> Dict:
        """
        Execute une requ√™te RAG compl√®te
        
        Args:
            user_query: Question de l'utilisateur
            return_sources: Retourner les sources utilis√©es
            stream: Streaming de la r√©ponse
            
        Returns:
            Dictionnaire avec r√©ponse et m√©tadonn√©es
        """
        print(f"\n{'='*60}")
        print(f"  Question: {user_query}")
        print(f"{'='*60}\n")
        
        # 1. RETRIEVAL: R√©cup√©rer les chunks pertinents
        print("üîç Phase 1: R√©cup√©ration des documents...")
        retrieved_chunks = self.retriever.retrieve_with_reranking(user_query)
        
        if not retrieved_chunks:
            return {
                'answer': "Je n'ai trouv√© aucune information pertinente pour r√©pondre √† votre question.",
                'sources': [],
                'num_chunks_used': 0
            }
        
        # 2. FORMATTING: Cr√©er le prompt
        print("  Phase 2: Formatage du contexte...")
        context = self._format_context(retrieved_chunks)
        
        prompt = self.system_prompt.format(
            context=context,
            query=user_query
        )
        
        # 3. GENERATION: G√©n√©rer la r√©ponse
        print("  Phase 3: G√©n√©ration de la r√©ponse...\n")
        answer = self.llm.generate(prompt, stream=stream)
        
        # 4. FORMAT RESPONSE
        response = {
            'answer': answer.strip(),
            'num_chunks_used': len(retrieved_chunks)
        }
        
        if return_sources:
            response['sources'] = [
                {
                    'source': chunk['metadata'].get('source', 'Inconnu'),
                    'score': chunk.get('similarity_score', 0),
                    'preview': chunk['content'][:200] + "..."
                }
                for chunk in retrieved_chunks
            ]
        
        return response
    
    def interactive_chat(self):
        """Mode chat interactif"""
        print("\n" + "="*60)
        print("  ESILV Smart Assistant - Mode Chat")
        print("="*60)
        print("Tapez 'quit' pour quitter\n")
        
        while True:
            user_input = input("  Vous: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("  Au revoir!")
                break
            
            if not user_input:
                continue
            
            # Traiter la requ√™te
            result = self.query(user_input, return_sources=False, stream=True)
            
            print(f"\n  Chunks utilis√©s: {result['num_chunks_used']}\n")