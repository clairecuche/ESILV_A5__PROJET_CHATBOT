from typing import List, Dict, Optional
from src.rag.generation.llm_handler import OllamaLLM
from src.rag.generation.retriever_lang import Retriever

class RAGPipeline:
    """
    Pipeline RAG complet: Retrieval + Generation
    """
    
    def __init__(
        self,
        retriever: Retriever,
        llm: OllamaLLM,
        system_prompt: Optional[str] = None
    ):
        """
        Args:
            retriever: SystÃ¨me de rÃ©cupÃ©ration
            llm: ModÃ¨le de langage Ollama
            system_prompt: Instructions systÃ¨me personnalisÃ©es
        """
        self.retriever = retriever
        self.llm = llm
        
        self.system_prompt = system_prompt or self._default_system_prompt()
    
    def _default_system_prompt(self) -> str:
        """Prompt systÃ¨me par dÃ©faut pour ESILV"""
        return """Tu es un assistant virtuel expert de l'Ã©cole d'ingÃ©nieurs ESILV.

Ton rÃ´le :
- RÃ©pondre aux questions sur ESILV en te basant UNIQUEMENT sur les documents fournis
- ÃŠtre prÃ©cis, factuel et utile
- Citer tes sources quand possible

RÃ¨gles importantes :
1. Utilise UNIQUEMENT les informations des documents fournis dans le contexte
2. Si l'information n'est pas dans les documents, dis "Je n'ai pas cette information dans ma documentation"
3. Ne jamais inventer ou supposer des informations
4. Reste professionnel mais chaleureux
5. Si pertinent, suggÃ¨re Ã  l'utilisateur d'Ãªtre contactÃ© pour plus de dÃ©tails

RÃ¨gles de citation des sources :
- CITE la source UNIQUEMENT si c'est un lien web (commence par http:// ou https://)
- N'affiche JAMAIS les chemins de fichiers internes (ex: data//pdf//..., //documents//...)
- Format pour les liens web : "Source : [URL]" ou "Plus d'infos : [URL]"
- Si toutes les sources sont des fichiers internes, ne mentionne aucune source

Format de rÃ©ponse :
- RÃ©ponds de maniÃ¨re claire et structurÃ©e
- Utilise des listes Ã  puces si appropriÃ©
- Place les liens web sources Ã  la fin de ta rÃ©ponse si applicable

CONTEXTE:
{context}

---

QUESTION: {query}

RÃ‰PONSE:"""
    
    def _format_context(self, chunks: List[Dict]) -> str:
        """
        Formate les chunks rÃ©cupÃ©rÃ©s en contexte structurÃ© pour le LLM
        
        Args:
            chunks: Liste de chunks avec leurs mÃ©tadonnÃ©es et scores
            
        Returns:
            Contexte formatÃ© et numÃ©rotÃ©
        """
        if not chunks:
            return "Aucun contexte disponible."
        
        context_parts = []
        
        for i, chunk in enumerate(chunks, 1):
            # RÃ©cupÃ©ration des mÃ©tadonnÃ©es
            source = chunk['metadata'].get('source', 'Document inconnu')
            page = chunk['metadata'].get('page', 'N/A')
            
            # RÃ©cupÃ©ration du score final 
            final_score = chunk['scores']['final']
            
            # Nettoyage du contenu
            content = chunk['content'].strip()
            
            # Format clair et structurÃ©
            context_parts.append(
                f"--- DOCUMENT {i} ---\n"
                f"Source: {source} | Page: {page} | Pertinence: {final_score:.2f}\n\n"
                f"{content}\n"
            )
        
        return "\n".join(context_parts)
    
    def query(
        self,
        user_query: str,
        return_sources: bool = True,
        stream: bool = False,
        debug: bool = False
    ) -> Dict:
        """
        ExÃ©cute une requÃªte RAG complÃ¨te
        
        Args:
            user_query: Question de l'utilisateur
            return_sources: Retourner les sources utilisÃ©es
            stream: Streaming de la rÃ©ponse
            debug: Afficher le contexte envoyÃ© au LLM
            
        Returns:
            Dictionnaire avec rÃ©ponse et mÃ©tadonnÃ©es
        """
        print(f"\n{'='*60}")
        print(f"  Question: {user_query}")
        print(f"{'='*60}\n")
        
        # 1. RETRIEVAL: RÃ©cupÃ©rer les chunks pertinents
        print("Phase 1: RÃ©cupÃ©ration des documents...")
        retrieved_chunks = self.retriever.retrieve_with_reranking(
            user_query, 
            debug=debug
        )
        
        if not retrieved_chunks:
            return {
                'answer': "Je n'ai trouvÃ© aucune information pertinente pour rÃ©pondre Ã  votre question.",
                'sources': [],
                'num_chunks_used': 0
            }
        
        # 2. FORMATTING: CrÃ©er le contexte structurÃ©
        print("Phase 2: Formatage du contexte...")
        context = self._format_context(retrieved_chunks)
        
        # Debug: afficher le contexte exact envoyÃ© au LLM
        
       # print("\n" + "="*60)
       # print("CONTEXTE ENVOYÃ‰ AU LLM:")
      #  print("="*60)
      #  print(context)
      #  print("="*60 + "\n")
        
        # Construire le prompt complet
        prompt = self.system_prompt.format(
            context=context,
            query=user_query
        )
        
        # 3. GENERATION: GÃ©nÃ©rer la rÃ©ponse
        print("Phase 3: GÃ©nÃ©ration de la rÃ©ponse...\n")
        answer = self.llm.generate(prompt, stream=stream)
        
        # 4. FORMAT RESPONSE
        response = {
            'answer': answer.strip(),
            'num_chunks_used': len(retrieved_chunks)
        }
        
        if return_sources:
            response['sources'] = [
                {
                    'source': chunk['metadata'].get('source', 'Inconnu'),
                    'page': chunk['metadata'].get('page', 'N/A'),
                    'final_score': chunk['scores']['final'],
                    'vector_score': chunk['scores']['vector'],
                    'lexical_score': chunk['scores']['lexical'],
                    'preview': chunk['content'][:250] + "..." if len(chunk['content']) > 250 else chunk['content']
                }
                for chunk in retrieved_chunks
            ]
        
        return response
        
    def interactive_chat(self, debug: bool = False):
        """Mode chat interactif"""
        print("\n" + "="*60)
        print("  ESILV Smart Assistant - Mode Chat")
        print("="*60)
        print("Commandes: 'quit' pour quitter, 'debug' pour toggle debug\n")
        
        current_debug = debug
        
        while True:
            user_input = input("  Vous: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("  Au revoir!")
                break
            
            if user_input.lower() == 'debug':
                current_debug = not current_debug
                print(f"  ğŸ”§ Debug mode: {'ON' if current_debug else 'OFF'}\n")
                continue
            
            if not user_input:
                continue
            
            # Traiter la requÃªte
            result = self.query(
                user_input, 
                return_sources=False, 
                stream=True,
                debug=current_debug
            )
            
            print(f"\n  ğŸ“š {result['num_chunks_used']} chunks utilisÃ©s\n")
    