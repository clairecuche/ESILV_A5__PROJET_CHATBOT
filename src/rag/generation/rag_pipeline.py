from typing import List, Dict, Optional
from src.rag.generation.llm_handler import OllamaLLM
from src.rag.generation.retriever_lang import Retriever

class RAGPipeline:
    """
    Pipeline RAG complet: Retrieval + Generation
    """
    
    def __init__(
        self,
        retriever: Retriever,
        llm: OllamaLLM,
        system_prompt: Optional[str] = None
    ):
        """
        Args:
            retriever: SystÃ¨me de rÃ©cupÃ©ration
            llm: ModÃ¨le de langage Ollama
            system_prompt: Instructions systÃ¨me personnalisÃ©es
        """
        self.retriever = retriever
        self.llm = llm
        
        self.system_prompt = system_prompt or self._default_system_prompt()
    
    def _default_system_prompt(self) -> str:
        """Prompt systÃ¨me par dÃ©faut pour ESILV"""
        return """Tu es un assistant intelligent spÃ©cialisÃ© pour l'Ã©cole d'ingÃ©nieurs ESILV (Ã‰cole SupÃ©rieure d'IngÃ©nieurs LÃ©onard de Vinci).

RÃˆGLES IMPORTANTES:
1. Base ta rÃ©ponse UNIQUEMENT sur les informations prÃ©sentes dans le CONTEXTE ci-dessous
2. Si l'information est dans le contexte (mÃªme partiellement ou noyÃ©e dans un paragraphe), EXTRAIS-LA et formule une rÃ©ponse claire
3. RÃ©ponds de maniÃ¨re directe et prÃ©cise Ã  la question posÃ©e
4. Si l'information est TOTALEMENT absente du contexte, dis: "Je n'ai pas trouvÃ© cette information dans les documents fournis."
5. Ne JAMAIS inventer ou dÃ©duire des informations qui ne sont pas explicitement mentionnÃ©es
6. Utilise un ton professionnel mais amical
7. RÃ©ponds en franÃ§ais

CONTEXTE:
{context}

---

QUESTION: {query}

RÃ‰PONSE:"""
    
    def _format_context(self, chunks: List[Dict]) -> str:
        """
        Formate les chunks rÃ©cupÃ©rÃ©s en contexte structurÃ© pour le LLM
        
        Args:
            chunks: Liste de chunks avec leurs mÃ©tadonnÃ©es et scores
            
        Returns:
            Contexte formatÃ© et numÃ©rotÃ©
        """
        if not chunks:
            return "Aucun contexte disponible."
        
        context_parts = []
        
        for i, chunk in enumerate(chunks, 1):
            # RÃ©cupÃ©ration des mÃ©tadonnÃ©es
            source = chunk['metadata'].get('source', 'Document inconnu')
            page = chunk['metadata'].get('page', 'N/A')
            
            # RÃ©cupÃ©ration du score final 
            final_score = chunk['scores']['final']
            
            # Nettoyage du contenu
            content = chunk['content'].strip()
            
            # Format clair et structurÃ©
            context_parts.append(
                f"--- DOCUMENT {i} ---\n"
                f"Source: {source} | Page: {page} | Pertinence: {final_score:.2f}\n\n"
                f"{content}\n"
            )
        
        return "\n".join(context_parts)
    
    def query(
        self,
        user_query: str,
        return_sources: bool = True,
        stream: bool = False,
        debug: bool = False
    ) -> Dict:
        """
        ExÃ©cute une requÃªte RAG complÃ¨te
        
        Args:
            user_query: Question de l'utilisateur
            return_sources: Retourner les sources utilisÃ©es
            stream: Streaming de la rÃ©ponse
            debug: Afficher le contexte envoyÃ© au LLM
            
        Returns:
            Dictionnaire avec rÃ©ponse et mÃ©tadonnÃ©es
        """
        print(f"\n{'='*60}")
        print(f"  Question: {user_query}")
        print(f"{'='*60}\n")
        
        # 1. RETRIEVAL: RÃ©cupÃ©rer les chunks pertinents
        print("Phase 1: RÃ©cupÃ©ration des documents...")
        retrieved_chunks = self.retriever.retrieve_with_reranking(
            user_query, 
            debug=debug
        )
        
        if not retrieved_chunks:
            return {
                'answer': "Je n'ai trouvÃ© aucune information pertinente pour rÃ©pondre Ã  votre question.",
                'sources': [],
                'num_chunks_used': 0
            }
        
        # 2. FORMATTING: CrÃ©er le contexte structurÃ©
        print("Phase 2: Formatage du contexte...")
        context = self._format_context(retrieved_chunks)
        
        # Debug: afficher le contexte exact envoyÃ© au LLM
        if debug:
            print("\n" + "="*60)
            print("CONTEXTE ENVOYÃ‰ AU LLM:")
            print("="*60)
            print(context)
            print("="*60 + "\n")
        
        # Construire le prompt complet
        prompt = self.system_prompt.format(
            context=context,
            query=user_query
        )
        
        # 3. GENERATION: GÃ©nÃ©rer la rÃ©ponse
        print("Phase 3: GÃ©nÃ©ration de la rÃ©ponse...\n")
        answer = self.llm.generate(prompt, stream=stream)
        
        # 4. FORMAT RESPONSE
        response = {
            'answer': answer.strip(),
            'num_chunks_used': len(retrieved_chunks)
        }
        
        if return_sources:
            response['sources'] = [
                {
                    'source': chunk['metadata'].get('source', 'Inconnu'),
                    'page': chunk['metadata'].get('page', 'N/A'),
                    'final_score': chunk['scores']['final'],
                    'vector_score': chunk['scores']['vector'],
                    'lexical_score': chunk['scores']['lexical'],
                    'preview': chunk['content'][:250] + "..." if len(chunk['content']) > 250 else chunk['content']
                }
                for chunk in retrieved_chunks
            ]
        
        return response
        
    def interactive_chat(self, debug: bool = False):
        """Mode chat interactif"""
        print("\n" + "="*60)
        print("  ESILV Smart Assistant - Mode Chat")
        print("="*60)
        print("Commandes: 'quit' pour quitter, 'debug' pour toggle debug\n")
        
        current_debug = debug
        
        while True:
            user_input = input("  Vous: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("  Au revoir!")
                break
            
            if user_input.lower() == 'debug':
                current_debug = not current_debug
                print(f"  ğŸ”§ Debug mode: {'ON' if current_debug else 'OFF'}\n")
                continue
            
            if not user_input:
                continue
            
            # Traiter la requÃªte
            result = self.query(
                user_input, 
                return_sources=False, 
                stream=True,
                debug=current_debug
            )
            
            print(f"\n  ğŸ“š {result['num_chunks_used']} chunks utilisÃ©s\n")
    